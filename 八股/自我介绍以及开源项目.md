自我介绍：面试官您好，我叫吴希，来自西安邮电大学本科大三在读，在大一时，我加入了关于linux的实验室，至此之后我一直是在linux系统写代码，在大一上学期，我自学了c语言并实现了ls,shell等工具，在大一下学期，我学习了c++并阅读了linux网络编程这本书，并根据内容在暑假用c++写了一个基于epoll的多人聊天室，在大二上学期，由于对于操作系统比较感兴趣，我根据操作系统真想还原这本书从0实现了一个os,在大二下学期，我学习了go语言为了熟悉go语言，我用go仿照kafka写了一个消息队列，之后，为了提高自己编成开发和多人协作开发项目，在暑假我积极竞选中科院开源之下项目，最终中选了kmesh社区关于给ebpf写ut测试的项目并认真完成。

开源项目，面试官您手下留情呀

kmesh是一个基于ebpf技术的服务网格数据面开源项目，目的是为了解决传统服务网格在性能损耗和资源占用上的缺点，他的核心创新点是通过ebpf在内核层实现流量拦截和服务治理，绕过了用户态代理，实现接近原声网络的性能。由于对于ebpf程序缺乏单元测试导致代码质量风险高，迭代效率低，我的任务主要是实现对sendmsg和cgroup两类关键内核态程序的测试用例，sendmsg主要是检测在收发消息时在消息头前加的tlv格式内容是否正确，cgroup分为cgroup_skb和cgroup_sock的实现，cgroup_skb主要是检测数据包并在特定情况下触发发送日志，cgroup_sock主要是在 **新建连接时** 做服务发现 & 负载均衡，主要检测连接时是否正确 **重定向目标地址**，负载均衡策略是否生效，我的测试主要的测试是用c和go编写的，c代码主要负载Mock无法触发的程序，go代码主要通过真实网络行为来验证Ebpf处理结果是否符合预期。

那么面试官肯定会问，传统的服务网格的性能瓶颈是什么，kmesh是如何进行优化的？

传统的每个pod旁边都会部署一个Sidercar代理，应用流量需要经过用户态代理，内核态，用户态，应用。首先应用发包到内核网格栈，内核把流量交给用户态Sidecar,sidecar再做l7处理。然后把流量协会内核网格栈，在到达目标pod.二ebpf程序他是直接在内核中完成的，通过将他挂载到socket或者skb hook,流量治理逻辑直接在内核态完成，不需要sider,不需要流量上下文切换到内核态。性能方面，因为流量直接从内核网格栈直接经过ebpf程序的处理，不用到用户态代理，减少了上下文切换。不需要给每个Pod部署Envoy,减少了内存的占用

ai:

**触发链路**：控制面把服务/LB 策略写入 BPF maps；应用 `connect` 触发 **cgroup_sock**，我们查 map 选后端并**直接改 ctx 的目的地址**完成“建连即 DNAT”；发送时 **sk_msg** 在 payload 前**注入 TLV**（如原目的），必要时可 **重定向**；收发包则由 **cgroup_skb** 做**观测与上报**。
 **收益**：全程在 **内核态** 完成，**绕过 Sidecar**，减少上下文切换与内存开销，性能接近原生网络；策略热更新只改 map，稳定且可扩展。

那么面试管肯定会问，你了解docker和k8s吗？

ebpf程序需要在linux内核运行，直接在宿主机测试风险太高，那么就创建了docker来加载环境，kmesh需要用k8s集群来验证流量拦截和服务治理等，当时开始项目时我学会用kind创建集群，并创建

那你任务kmesh为什么要引入ebpf?

ebpf是linux内核的沙箱式虚拟机，允许安全的在内核中运行用户定义的程序，引入他主要有几个优点，首先数据在内核层直接处理，避免了内核态到用户态的数据拷贝，其次因为ebpf直接在内核内完成减少了上下文切换。

ebpf的maps你了解马？

他的主要作用，是内核态和用户态数据交换的存储结构

--------------------------------------------------重新来过这次要号好看看开源，不要回答的冉冉的

**介绍一下kmesh社区以及你做的任务：**kmesh社区他主要是基于ebpf的服务网格，传统的服务网格是通过istio当作控制面板，然后用户态的envoy当作数据面，而现在就是kmesh在渐渐的取代envoy，kmesh作为数据面，与istio结合成为新的服务网格，那么为什么会有kmesh的出现呢，主要就是传统的服务网格他用户态的流量通过envoy进行流量转发他是需要进行多次的用户态和内核太的上下文切换，这样会很影响性能，而且就是在k8s集群中部署的话，每个节点中的多个pod他都需要由一个envoy,但是对于kmesh来说，就是一个节点他只需要部署一个kmesh就可以，也节省了大量内存。

最主流的就是lstio+envoy+k8s集群

envoy传统的整个业务逻辑【比如说用户点击app】:

从用户端的视角来看：

在购物app上点击“查看商品库存”，手机会发送一个：要查看这个商品库层“的请求，会先到电商公司服务端k8s里的ingress gateway,这个ingress gateway里面的envoy首先会根据lstio规则，看他是否可以进，比如说没登录的用户就不让查库存，然后就是根据lstio找到库存服务pod,并把这个请求转发的对应得pod，那么请求现在到了pod,pod里面的库存服务会察看这个商品还有多少库存然后返回给这个pod里面的envoy,然后这个pod的envoy再将拿到的数据进行原路返回到ingress的envoy,r然后由ingressgate里的envoy发送给你的手机。

但是在这个过程中会导致大量的上下文切换，主要就是因为envoy他是用户态代理，他所有的收发数据都必须通过网卡，所以每次的envoy接收到envoy转发都会触发内核太用户态内核态的切换

ai:

主要是因为 Envoy 是用户态代理，它本身没有直接操作硬件（比如网卡）的权限，所有收发数据都必须通过操作系统内核（内核态）来完成。所以每次 Envoy 接收数据（内核态→用户态）、转发数据（用户态→内核态），都会触发两次上下文切换，整个流程下来切换次数就会累积得很多

那么kmesh就诞生了，主要就是为了逐渐代替envoy内核态和用户态频繁进行上下文切换导致的性能问题。

用户态感受：

在用户态点击查看商品库存的时候，也是先到电商服务k8s的ingress gateway里面，但是上面的不同点是ingress gateway的流量检验，以及后续与pod的库存服务进行交互都是通过kmesh来完成的。

因为kmesh是基于ebpf的内核态方案，那么他的转发逻辑直接就嵌入在操作系统内核，必须要向envoy那样需要频繁摆脱内核处理数据

比如kmesh在接收到请求后，会直接在内核态完成哪个数据的检查，转发，不需要切换到用户态，那么就优化了上下文频繁切换导致的性能问题

那么我主要是编写了部分l4层的流量转发，就是分别是三个部分，sendmsg和cgroup_skb和cgroup_sock,这些使用c语言写的ebpf文件，在内核态进行触发，然后每个.c还对应一个,c文件，里面主要是响应需要mock的函数【什么函数需要mock】，然后在go端写了即成册是，也就是通过真实的挂载触发，然后收发网络包的一个方式来进行ebpf程序的触发，通过ebpf map通道/ringbuf将内核和用户串联起来，检验得到的数据是否正确

沟通方面：当时写完ebpf程序然后在写相应的测试的时候，kmesh时有两个测试框架的，bpf_prog_test_run和build_context,因为我要写的这个在搜查时好像不支持bpf_prog_test_run,然后我就选择build_context

加油，你是最棒的！！！

他们统一的流程时这样的：首先编写了ebpf代码，在编写的代码时候要执行相应的函数前面会加上SEC,比如说SEC("sk_msg")

【SEC的作用就是指定程序挂载的hook点，比如这个sk_msg就是socket消息的一个hook点的时候就会触发下面的程序执行】

由于ebpd程序时运行在内核态，那肯定就不能由普通的c编译器编译，当时时通过clang+bpftool等工具，在go端将ebpf的c代码编译成内核可以别的ebpf字节码.o文件，然后在go测试中，通过一个ebpf库，cilium/ebpf的ebpf.LoadCollectionSpec(.o)将.o文件的函数和map加载到内核，内核会对相应的函数和map进行初始化，这里的map就是内核和用户进行数据传递的桥梁

比如说用户在调用了sendmsg函数，他底层触发的是socket,那么他会在先入内核之后执行这个ebpf代码，并将相应的结果通过map进行传入给用户态

注意点：

`sk_msg`钩子正是内核处理`socket`消息时的一个执行点

这个和普通程序的.o文件有什么不同码

（而非普通 CPU 机器码），只能在 kernel 的 eBPF 虚拟机中运行，且自带严格的功能限制（比如禁止循环）；

**你是怎样在go端调用c的？**

1：编译：clang -target bpf 将c原代码编译成ebpf字节码.o

2：加载：调用go的cilium/ebpf的loadCollectionSepc和NewCollextion函数，将.o程序加载入内核【这些函数运行完之后就会将.oebpf字节码加载到内核，并返回一个collection集合，这个集合是用户态管理ebpf资源的入口，他包含了加载到内核的所有程序和映射，后续也主要是通过他来操作内核资源】

3：获取ebpf程序和映射：比如说以我写的sendmag举例，上面已经获得了这个集合，那么他会通过调用coll.Programs["sendmsg_prog"]从集合中获取名为sendmsg_prog的ebpf映射，通过coll.Maps[map_of_kmesh_sendmsg]获取名为map_of_kmesh_sendmsg的ebpf映射，

4：附着：将ebpf称许挂载到内核的挂载点，让内核在特定事件发生就自动执行改ebpf程序。通过link.attachprogram将上面的sockmap的fd,以及上面的sendsmg_prog这个ebpf程序进行关联。

那么将真实四元组的fd加入到这个sockmap中，要是在运行sendmsg，他查看这个fs是否属于sockmap,属于的话就会继续运行和这个sockmap有联系的这个hook点也就是send_prog

5：触发：用真实的socket 建立连接收发数据包，ebpf在内核态运行

6：交互验证：go端通过map/ringbuf对去结果并根据逻辑去进行检验

```
spec, _ := ebpf.LoadCollectionSpec(objPath)
coll, _ := ebpf.NewCollection(spec)
prog := coll.Programs["sendmsg_prog"]
sockMap := coll.Maps["map_of_kmesh_sendmsg"]
// 附着 sk_msg 到 sockhash
_ = link.RawAttachProgram(link.RawAttachProgramOptions{
  Attach:  ebpf.AttachSkMsgVerdict,
  Target:  sockMap.FD(),
  Program: prog,
})
// 将连接的四元组→fd 写入 sockhash，随后真实 send 触发 eBPF
_ = sockMap.Update(&tupleKey, &fd32, ebpf.UpdateAny)
```

这prog具体来说并不是ebpf中的sendmsg_prog,而是这个ebpf程序的一个文件描述符

ai:

 更准确的是：当 socket（其 FD 已写入`sockMap`）执行`sendmsg`操作时，内核会进入`SkMsgVerdict`钩子，此时发现该 socket 的 FD 在`sockMap`中，且`sockMap`已关联了`sendmsg_prog`程序，因此会触发该 eBPF 程序执行

**TLV**

TLV目前带有原始目的ip+端口

为什么要加tlv,靠tcp不就行了吗？【他是在tcp前面加】

首先tcp头部他不包含这些信息，然后就是在被透明代理劫持之后，只能看到目前想要去的地址，无法找到自己的原始目标地址。

比如说原始是10.0.1.10：54321-》10.1.2.3：80/tcp,现在被代理劫持之后，他只能看到10.0.1.10：54321-》169.254.0.1：15006/tcp,那么就不知道他追出想要区的目标地址是哪里，要是这个经过路由的去不了，那么他也会不到原来的去了因为他没有被记录

还有就是他的长度是可以变化的，那么想添加需要的信息就非常的灵活。ai:TLV 使用 Type/Length/Value，自描述、可扩展。

**具体做了啥：**

sendmsg:在消息前面在加上自己想要的信息

cgroup_skb:主要是做数据的监测并通过ringbuf上报信息给go测试段

cgroup_sock:负责流量的转发和负载均衡。主要就是他会去查backend的waypoint是否是1,要是是就直接读取转发backend的就可以，要是不行就需要依据service里面的policy进行选择，主要就是本地优先，远程优先，远程优先，找不到就本地这些策略，来进行负载均衡的队对应目的服务

具体应该是这样：

cgroup_sock:他主要就是先通过前端的frontend来选择连接的上有id是backend还是service

要是是backend就看他的waypoine是不是1,要是1,要是是才能转到对应的目标地址，要是不是，就直接用原来的原始目标地址

要是是service,要是他的waypoint是1,那么也是可以直接转到队赢得目标地址，要是不是，就需要通过policy进行选择

一般是根据优先级选择本地优先=0/远程优先=1/本地优先，没有的话走远程，他主要是先朝优先级是0的，要是没有的话就回退到6,这三个政策来进行选择，选择完政策后，，随机挑选一个满足条件的优先级，找到他们对应得backend,然后在看他们的waypoine是不会1,要是是就进行转发，要是不是还是用原始目标地址





























