自我介绍：面试官您好，我叫吴希，来自西安邮电大学本科大三在读，在大一时，我加入了关于linux的实验室，至此之后我一直是在linux系统写代码，在大一上学期，我自学了c语言并实现了ls,shell等工具，在大一下学期，我学习了c++并阅读了linux网络编程这本书，并根据内容在暑假用c++写了一个基于epoll的多人聊天室，在大二上学期，由于对于操作系统比较感兴趣，我根据操作系统真想还原这本书从0实现了一个os,在大二下学期，我学习了go语言为了熟悉go语言，我用go仿照kafka写了一个消息队列，之后，为了提高自己编成开发和多人协作开发项目，在暑假我积极竞选中科院开源之下项目，最终中选了kmesh社区关于给ebpf写ut测试的项目并认真完成。

开源项目，面试官您手下留情呀

kmesh是一个基于ebpf技术的服务网格数据面开源项目，目的是为了解决传统服务网格在性能损耗和资源占用上的缺点，他的核心创新点是通过ebpf在内核层实现流量拦截和服务治理，绕过了用户态代理，实现接近原声网络的性能。由于对于ebpf程序缺乏单元测试导致代码质量风险高，迭代效率低，我的任务主要是实现对sendmsg和cgroup两类关键内核态程序的测试用例，sendmsg主要是检测在收发消息时在消息头前加的tlv格式内容是否正确，cgroup分为cgroup_skb和cgroup_sock的实现，cgroup_skb主要是检测数据包并在特定情况下触发发送日志，cgroup_sock主要是在 **新建连接时** 做服务发现 & 负载均衡，主要检测连接时是否正确 **重定向目标地址**，负载均衡策略是否生效，我的测试主要的测试是用c和go编写的，c代码主要负载Mock无法触发的程序，go代码主要通过真实网络行为来验证Ebpf处理结果是否符合预期。

那么面试官肯定会问，传统的服务网格的性能瓶颈是什么，kmesh是如何进行优化的？

传统的每个pod旁边都会部署一个Sidercar代理，应用流量需要经过用户态代理，内核态，用户态，应用。首先应用发包到内核网格栈，内核把流量交给用户态Sidecar,sidecar再做l7处理。然后把流量协会内核网格栈，在到达目标pod.二ebpf程序他是直接在内核中完成的，通过将他挂载到socket或者skb hook,流量治理逻辑直接在内核态完成，不需要sider,不需要流量上下文切换到内核态。性能方面，因为流量直接从内核网格栈直接经过ebpf程序的处理，不用到用户态代理，减少了上下文切换。不需要给每个Pod部署Envoy,减少了内存的占用

ai:

**触发链路**：控制面把服务/LB 策略写入 BPF maps；应用 `connect` 触发 **cgroup_sock**，我们查 map 选后端并**直接改 ctx 的目的地址**完成“建连即 DNAT”；发送时 **sk_msg** 在 payload 前**注入 TLV**（如原目的），必要时可 **重定向**；收发包则由 **cgroup_skb** 做**观测与上报**。
 **收益**：全程在 **内核态** 完成，**绕过 Sidecar**，减少上下文切换与内存开销，性能接近原生网络；策略热更新只改 map，稳定且可扩展。

那么面试管肯定会问，你了解docker和k8s吗？

ebpf程序需要在linux内核运行，直接在宿主机测试风险太高，那么就创建了docker来加载环境，kmesh需要用k8s集群来验证流量拦截和服务治理等，当时开始项目时我学会用kind创建集群，并创建

那你任务kmesh为什么要引入ebpf?

ebpf是linux内核的沙箱式虚拟机，允许安全的在内核中运行用户定义的程序，引入他主要有几个优点，首先数据在内核层直接处理，避免了内核态到用户态的数据拷贝，其次因为ebpf直接在内核内完成减少了上下文切换。

ebpf的maps你了解马？

他的主要作用，是内核态和用户态数据交换的存储结构

















































