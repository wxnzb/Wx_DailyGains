自我介绍：面试官您好，我叫吴希，来自西安邮电大学本科大三在读，在大一时，我加入了关于linux的实验室，至此之后我一直是在linux系统写代码，在大一上学期，我自学了c语言并实现了ls,shell等工具，在大一下学期，我学习了c++并阅读了linux网络编程这本书，并根据内容在暑假用c++写了一个基于epoll的多人聊天室，在大二上学期，由于对于操作系统比较感兴趣，我根据操作系统真想还原这本书从0实现了一个os,在大二下学期，我学习了go语言为了熟悉go语言，我用go仿照kafka写了一个消息队列，之后，为了提高自己编成开发和多人协作开发项目，在暑假我积极竞选中科院开源之下项目，最终中选了kmesh社区关于给ebpf写ut测试的项目并认真完成。

开源项目，面试官您手下留情呀

kmesh是一个基于ebpf技术的服务网格数据面开源项目，目的是为了解决传统服务网格在性能损耗和资源占用上的缺点，他的核心创新点是通过ebpf在内核层实现流量拦截和服务治理，绕过了用户态代理，实现接近原声网络的性能。由于对于ebpf程序缺乏单元测试导致代码质量风险高，迭代效率低，我的任务主要是实现对sendmsg和cgroup两类关键内核态程序的测试用例，sendmsg主要是检测在收发消息时在消息头前加的tlv格式内容是否正确，cgroup分为cgroup_skb和cgroup_sock的实现，cgroup_skb主要是检测数据包并在特定情况下触发发送日志，cgroup_sock主要是在 **新建连接时** 做服务发现 & 负载均衡，主要检测连接时是否正确 **重定向目标地址**，负载均衡策略是否生效，我的测试主要的测试是用c和go编写的，c代码主要负载Mock无法触发的程序，go代码主要通过真实网络行为来验证Ebpf处理结果是否符合预期。

那么面试官肯定会问，传统的服务网格的性能瓶颈是什么，kmesh是如何进行优化的？

传统的每个pod旁边都会部署一个Sidercar代理，应用流量需要经过用户态代理，内核态，用户态，应用。首先应用发包到内核网格栈，内核把流量交给用户态Sidecar,sidecar再做l7处理。然后把流量协会内核网格栈，在到达目标pod.二ebpf程序他是直接在内核中完成的，通过将他挂载到socket或者skb hook,流量治理逻辑直接在内核态完成，不需要sider,不需要流量上下文切换到内核态。性能方面，因为流量直接从内核网格栈直接经过ebpf程序的处理，不用到用户态代理，减少了上下文切换。不需要给每个Pod部署Envoy,减少了内存的占用

ai:

**触发链路**：控制面把服务/LB 策略写入 BPF maps；应用 `connect` 触发 **cgroup_sock**，我们查 map 选后端并**直接改 ctx 的目的地址**完成“建连即 DNAT”；发送时 **sk_msg** 在 payload 前**注入 TLV**（如原目的），必要时可 **重定向**；收发包则由 **cgroup_skb** 做**观测与上报**。
 **收益**：全程在 **内核态** 完成，**绕过 Sidecar**，减少上下文切换与内存开销，性能接近原生网络；策略热更新只改 map，稳定且可扩展。

那么面试管肯定会问，你了解docker和k8s吗？

ebpf程序需要在linux内核运行，直接在宿主机测试风险太高，那么就创建了docker来加载环境，kmesh需要用k8s集群来验证流量拦截和服务治理等，当时开始项目时我学会用kind创建集群，并创建

那你任务kmesh为什么要引入ebpf?

ebpf是linux内核的沙箱式虚拟机，允许安全的在内核中运行用户定义的程序，引入他主要有几个优点，首先数据在内核层直接处理，避免了内核态到用户态的数据拷贝，其次因为ebpf直接在内核内完成减少了上下文切换。

ebpf的maps你了解马？

他的主要作用，是内核态和用户态数据交换的存储结构

--------------------------------------------------重新来过这次要号好看看开源，不要回答的冉冉的

介绍一下kmesh社区以及你做的任务：kmesh社区他主要是基于ebpf的服务网格，传统的服务网格是通过istio当作控制面板，然后用户态的envoy当作数据面，而现在就是kmesh在渐渐的取代envoy，kmesh作为数据面，与istio结合成为新的服务网格，那么为什么会有kmesh的出现呢，主要就是传统的服务网格他用户态的流量通过envoy进行流量转发他是需要进行多次的用户态和内核太的上下文切换，这样会很影响性能，而且就是在k8s集群中部署的话，每个节点中的多个pod他都需要由一个envoy,但是对于kmesh来说，就是一个节点他只需要部署一个kmesh就可以，也节省了大量内存。

最主流的就是lstio+envoy+k8s集群

envoy传统的整个业务逻辑【比如说用户点击app】:

从用户端的视角来看：

在购物app上点击“查看商品库存”，手机会发送一个：要查看这个商品库层“的请求，会先到电商公司服务端k8s里的ingress gateway,这个ingress gateway里面的envoy首先会根据lstio规则，看他是否可以进，比如说没登录的用户就不让查库存，然后就是根据lstio找到库存服务pod,并把这个请求转发的对应得pod，那么请求现在到了pod,pod里面的库存服务会察看这个商品还有多少库存然后返回给这个pod里面的envoy,然后这个pod的envoy再将拿到的数据进行原路返回到ingress的envoy,r然后由ingressgate里的envoy发送给你的手机。

但是在这个过程中会导致大量的上下文切换，主要就是因为envoy他是用户态代理，他所有的收发数据都必须通过网卡，所以每次的envoy接收到envoy转发都会触发内核太用户态内核态的切换

ai:

主要是因为 Envoy 是用户态代理，它本身没有直接操作硬件（比如网卡）的权限，所有收发数据都必须通过操作系统内核（内核态）来完成。所以每次 Envoy 接收数据（内核态→用户态）、转发数据（用户态→内核态），都会触发两次上下文切换，整个流程下来切换次数就会累积得很多

那么kmesh就诞生了，主要就是为了逐渐代替envoy内核态和用户态频繁进行上下文切换导致的性能问题。

用户态感受：

在用户态点击查看商品库存的时候，也是先到电商服务k8s的ingress gateway里面，但是上面的不同点是ingress gateway的流量检验，以及后续与pod的库存服务进行交互都是通过kmesh来完成的。

因为kmesh是基于ebpf的内核态方案，那么他的转发逻辑直接就嵌入在操作系统内核，必须要向envoy那样需要频繁摆脱内核处理数据

比如kmesh在接收到请求后，会直接在内核态完成哪个数据的检查，转发，不需要切换到用户态，那么就优化了上下文频繁切换导致的性能问题

那么我主要是编写了部分l4层的流量转发，就是分别是三个部分，sendmsg和cgroup_skb和cgroup_sock,这些使用c语言写的ebpf文件，在内核态进行触发，然后每个.c还对应一个,c文件，里面主要是响应需要mock的函数【什么函数需要mock】，然后在go端写了即成册是，也就是通过真实的挂载触发，然后收发网络包的一个方式来进行ebpf程序的触发，通过ebpf map通道/ringbuf将内核和用户串联起来，检验得到的数据是否正确

沟通方面：当时写完ebpf程序然后在写相应的测试的时候，kmesh时有两个测试框架的，bpf_prog_test_run和build_context,因为我要写的这个在搜查时好像不支持bpf_prog_test_run,然后我就选择build_context

加油，你是最棒的！！！

他们统一的流程时这样的：首先编写了ebpf代码，在编写的代码时候要执行相应的函数前面会加上SEC,比如说SEC("sk_msg")

【SEC的作用就是指定程序挂载的hook点，比如这个sk_msg就是socket消息的一个hook点的时候就会触发下面的程序执行】

由于ebpd程序时运行在内核态，那肯定就不能由普通的c编译器编译，当时时通过clang+bpftool等工具，在go端将ebpf的c代码编译成内核可以别的ebpf字节码.o文件，然后在go测试中，通过一个ebpf库，cilium/ebpf的ebpf.LoadCollectionSpec(.o)将.o文件的函数和map加载到内核，内核会对相应的函数和map进行初始化，这里的map就是内核和用户进行数据传递的桥梁

比如说用户在调用了sendmsg函数，他底层触发的是socket,那么他会在先入内核之后执行这个ebpf代码，并将相应的结果通过map进行传入给用户态

注意点：

`sk_msg`钩子正是内核处理`socket`消息时的一个执行点

这个和普通程序的.o文件有什么不同码

（而非普通 CPU 机器码），只能在 kernel 的 eBPF 虚拟机中运行，且自带严格的功能限制（比如禁止循环）；









































