MQ

**面试管说，那你讲一下你mq的实现,你应该怎么说**

面试管您好，MQ是我开发的一个分布式消息中间件项目，目前基本支持分布式消息传递，RAft共识算法，zookeeper元数据管理，sstable存储引擎，高可用集群等核心功能，现在我详细讲一下我项目的整个流程。整个系统分为三层，分别是zookeeper元数据层做动态配置，Broker消息处理层，Client客户端层；Zookeeper元数据层，负责 Topic,Partition,Broker状态等配置管理以及故障检测与选举的协调，在Broker消息处理层，负责消息的写入，存储，副本同步和分发。每个Broker上的Partition内部有Raft协议来保证一致性。在Client客户端层，Producer和Consumer通过RPC与Broker交互，支持点对点和发布订阅。

**单点故障怎么办？**

首先在我的mq中有两个心跳机制，要是没有按时收到心跳，那么就说明有故障产生了。第一个 leader broker会定时向订阅他下面这个partition的消费者发送心跳，消费者要是没在一定时间内进行回复，那么leader broker会认为这个消费者挂了，他就会将这个消费者的扎状态由alive编成down,，那么要是有新消息到来时，就不会退送给这个消费者了；那么还有就是leader broker会定时向下面的follower broker发送心跳，要是follower在定时内没有收到leader broker的心跳，那么follower broker会认为leader broker已经宕机了，那么他会变成竞选者像其他follower发送投票请求，当收到一半同意时他就会变成leader

**你是如何保证数据的一致性和高可用性的？**

首先我通过raft共识算法，每个Partition都有一个独立的Raft集群，保证写入数据的强一致性；要是Leadre发生宕机，Follower会在超时后发起选举，选举出新的Leader即可，为了防止多个Follower同时发起选举，采用选举超时随机化来提高选举成功率。写入消息时，可以选择ack，Leader先会写入本地日志，在复制给Follower,然后返回；添加了快照机制。

你的mq是怎样实现高可用高可用性的？

首先raft的多数派原则保证只要超过一般节点存活，集群就继续工作，即使leader宕机，follower会在超时后自动发起选举，快速完成新的leader,完成自动故障转移；其次，数据写入时必须复制到多数派节点才算提交，这样可以保证即使少数节点当即，数据也不会丢失，新leader仍然可以提供服务。

**你的mq是怎么实现强一致性的？**

raft的一致性问题可以分成3个子问题，分别是leader选举，日志同步和安全性保证。对于leader选举，leader会定时向follower发送心跳，follower内部会维护一个随机超时时间的定时器，当在定时器超时之前收到来自leader的心跳，会将定时器时间置0并继续保持follower状态,要是在定时器超时没有收到来自leader的心跳，那么follower会认为leader宕机，follower会变成竞选者，然后follower会将自己的term+1,向其他follower发送自己想成为leader的请求，携带自己日志的term和index,其他follower进行检查，看是否他的term比自己的大，要是一样，就继续比较最后一条日志的term和index,要是满足投票条件，follower会向候选者发送投票，要是收到超过一半follower的同意，那么他就会成为leader并通知下面的follower自己成为了leader.对于日志同步，Leader将客户端请求加入到自己的日志中，然后并行向其他follower发送请求，其他follower将请求写入自己的日志之后并给leader回复自己已经复制，当leader接收到超过一半follower已经进行复制，那么这条日志会被标记为commit,接着他会执行这个请求并将执行结果返回给客户端；然后就是安全性保障，raft的多数投票规则，提交规则保证了raft的安全，他又两点，已经commit的消息一定会存在在后续leader节点中，但是没有commit的消息，就可能会丢失

**你是怎样优化存储引擎的？**

存储层面的结构是通过broker/topic/partition/block文件的层及，这样比较方便查找；我实现了类似LSM-Tree的存储结构，每个消息块里面有很多条消息，每个消息块前面有固定24字节的索引头，包含起始index,结束index,以及数据大小。这样消费者在查询offset消息是，不用从每条消息开始遍历，而是通过消息头进行块嗣定为，将查找复杂度直接从o(n)降低到了O(m);因为他是以追加的方式写入磁盘的，因此当消费者offset找的时候也比较好找；还有就是在存储的时候先写进内存队列，积累到消息阈值会将他进行刷盘操作，这样保证了性能又提高了安全；

**消息的发送机制**

生产者要发送消息时，他会先根据topic+partition来在本地先找有没有存储相对应的leaderbroker,要是没有，那么他会通过GetLeader来去zookeeper中找（这里zookeeper是怎么通过topic+partition来找到相应的leader broker的?___zookeeper他是通过topic+partition+nowblock拉获取到最新正在写的块的结构体，这里面就有对应的leader block)，找到后与leader broker建立连接，然后向这个leader broker发送消息,发送消息的时候我们会戴上ack,要是ack=-1,表示需要大多数follower同步复制并确认确认，根据ack=1是只要leader写入成功即可，ack=0则是开启一个携程进行异步副本写入操作。我们则一般选择的是ack=-1的形式。直接返回leader broker会先查询topic和partition是否存在，不存在就创建并启动partition接收消息，然后leader broker获得他下面的Followe broker,将消息发送给他们，他们将消息存储在本地并返回给leadre broker,leader broker看到有超过一半存储到partition 副本成功改就也存储在自己的partition下面，然后将结果返回给客户端生产者

**那对于消费者消费消息呢？整个流程**，加油，你是最棒的！！！

消费者消费消息由两种方式，一种是订阅一个topic/partition,那么要是topic下面的partition有了消息会直接推送给消费者；还有一种是消费者自己想要取拉取消息的

对于第一种：他会向zkserver查询这个partition对应的broker,然后他会和broker建立连接，然后broker会先检查这个消费者是否或者，要是或者就可以继续推送消息

对于第二种【消费者控制消费速度】：他会在topic+partition+他所需要的偏移量，然后在带上自己所要消费消息的数量，

**要是面试官问，那你raft同步和异步(fetch的实现)**

就是在producer的时候，就会和zookeeper进行交互设置他的模式为fetch和副本数量，那么在zookeeper他会通过topic+partition来通过一致性哈希来获取到broker,然后将第一个broker当作leader broker,然后遍历 follower，向每个follower发送addfetch的命令，这里面传入的参数就有 laeder broker和所有的follower broker，每个follower broker再收到命令后，会判断是否是leader broker,要是是他会遍历其他的follower broker，准备接收其他follower的pull请求，要是是Follower broker,他会和leader broker建立连接去主动拉取消息

那么就是要是是fetch,那么就要用到一致性哈希，他是通过topic+partition来得到一个key,然后通过这个key得到paroducer需要的节点数量，但是现在leader broker会变，他会变成第一个获取到的 broker，

**那你是怎样通过一致性哈希选出用哪个broker的**

首先就是根据消息带着的topic+partition(这里需要在细心，你还不清楚是怎样算的)计算出消息key的哈希值，然后在哈希环上找大于等于哈希值最近的虚拟节点，虚拟节点对应真实的broker就是负责存储该消息的broker

**为什么要虚拟节点：**

通过一个broker对应多个虚拟节点，并将这些虚拟节点均匀的分布在哈希环上面，可以使数据更加均匀的分布在不同的broker(那么又有一问了，那leader节点呢，没用吗？和他们一样吗？）上面，当某个broker宕机时，我们只需要将他对应的虚拟节点删除，并把虚拟节点上的数据迁移到其他节点就行。主要提高负载均衡能力和容错性

**有哈希环吗，就是一个partiiton会对应的虚拟节点，然后虚拟节点会找到真正的leader,那么既然一个partition对应一个raft集群，那他肯定虚拟节点最终找到的就还是他的leader节点吗，因此我不明白这个我添加的虚拟节点有什么用**【被问到了，一点不会】

他是生产者现在本地map里找有没有这个topic-partition对应的leaderB已经存起来了，要是有，就直接使用；要是没有，他会通过哈希环找到对应的broker,我默认是三个partition副本也就是会有3个broker,然后会对这三个broker上创建raft实例，他只是在partition首次创建时使用，用来决定这个partition由哪些broker来存储；然后在生产者生产了消息后，会直接向leader broker发送消息，然后leader broker会进行日志同步，每个follower broker在收到leader的消息后，会直接写入本地磁盘

那么虚拟节点的作用，主要时首先要是没有虚拟节点，每次创建一个prtition那么他会找broker进行分配，会产生节点分配不均匀的情况，就是有的节点他又3个partition,但是有的partition只有1个partition

然后还有一个问题就是要是新加一个节点，他要是用哈希环，可能上面的partition都要重新进行分配位，但是要是加了虚拟节点可能需要重新分配的partition数量就会少一点

```
原来：3个Broker，每个10个虚拟节点，环上共30个虚拟节点
新增Broker-D：添加10个虚拟节点，环上共40个虚拟节点

影响：
  只有新虚拟节点"顺时针下一个虚拟节点"对应的Partition需要迁移
  需要迁移的Partition数量：约25%（10/40）

示例：
  Broker-D-1插入到环上
  → 只有原本属于"Broker-D-1顺时针下一个虚拟节点"的Partition需要迁移过来
```



那还有哪些负载均衡算法？



**就是消费者消费了一条消息在磁盘中找不是还是很慢，有什么优化的方式吗**

**怎时使得消息不丢失**

**怎样保证分区容错性**

分区容错性就是系统能在网络不好的情况下依然能保证

**如何保证消息只被消费一次？**

ai:为了保证消息只被消费一次，针对不同的消息模式有不同的策略。在点对点（PTP）模式下，通过独占消费确保每个 partition 只能被一个 consumer 消费，顺序消费避免重复，原子更新 offset 保证消费进度正确，故障恢复后 consumer 从上次位置继续。在发布订阅（PSB）模式下，每个 consumer 维护独立的 offset，同一组内只有一个 consumer，不同组可以并行消费，消费进度持久化到 Zookeeper。通用的保证机制包括消息 ID 唯一性去重、状态跟踪（NOTDO -> HAVEDO -> HADDO）、确认机制确保消费成功才更新 offset、故障处理时检测故障并重新分配，以及持久化存储消费进度到 Zookeeper。

**kafka是怎样通过米等性来实现生产者只生产一次消息的？**

核心在于为每个生产者分配一个唯一的pid,针对每个topic+partition维护一个递增的序列号，当生产这发送消息是,broker会验证这个序列号是否是与环村中最新序列号连续，要是连续就进行更新并存储，要是重复就直接丢弃

**了解ack机制**

**消费者的推拉有什么区别**

LSM-Tree架构 = MemTable + 多层SSTable + Compaction机制

我实现了一个sstable存储格式

**那你讲一下lsm-tree**

是一种写入优化的存储结构，他的核心思想是写入内存，顺序写磁盘，合并来维护有序性。整体流程大概是这样，首先将数据顺序写入到wal文件中，防止突然断电导致消息丢失的情况，然后在将数据写入到memtable这个有序的红黑数中，当memtable写满后会变成immutable memtable,后台线程异步进行刷盘操作存储成sstable文件；这些sstable组成多层结构也就形成了lsm-tree;然后就是合并compaction，后台会将多个sstable合并成更大的sstable来减少文件的数量，并清理旧版本的key

要是你要查询一个数据，他会先查memtable,在查wal,要是wal没有，那么就说明肯定是很久远的消息，那么他会通过布隆过滤器来查询这个消息key可能在哪个sstable文件，因为一个key可能在多个sstable中，然后再这些sstable中用稀疏索引来快速定位文件内部的消息，

**你能说一下你的和lsm-tree有什么区别吗？**

我实现的主要是ssatble格式，他是lsm-tree架构中的一个核心组件，我借鉴了lsm-tree设计思想，实现了主要的消息块索引，顺序写入等功能，我只是将数据块顺序存储，但是lsm-tree的MemTable用跳表或红黑树等有序结构存储新写入的数据，还缺少lsm-tree关键的wal机制,这个主要是确保新写入的数据先记录到wal里面，即使断电了，也不会因为放在内存队列而消失的情况；以及lsm-tree还实现了多层结构会将文件从小的合并成大的，这样避免了文件数量一直增长的情况

**你是如何实现负载均衡和水平扩展的**

**如何解决消息堆积问题？消费者消费速度跟不上怎么办？**

可以从预防和解决两个角度触发解决消息挤压这个问题，对于预防，可以优化生产端，添加限流控制；也可以优化消费端，可以减少消费端不必要的i/o操作或者增加分区数量也就是体改并行能力；对于解决，可以直接扩大消费者数量

 **网络分区时，你的系统如何处理？会出现脑裂吗？**

**为什么选择kitex而不是gRPC？**

在我的mq项目中选择kitex而不选择grpc,首先是因为我的mq是用go实现的， Kitex 是专门为 Go 生态设计的高性能 RPC 框架；而且kitex他主要基于thrift写一和自即研究的netpoll网络库，在高并发，高体那兔场景夏延迟耕地，性能更优，非常契合消息队列这种对性能要求高的系统。对于grpc他主要是在跨语言和http/2通信的优势，对于我们mq的简单的请求响应，不需要实现这么复杂的，所以我觉得选用kitex更合适

ai:

“在我的 MQ 项目里，我选择 Kitex 而不是 gRPC，主要有两个原因：
 第一，Kitex 是专门为 Go 生态设计的高性能 RPC 框架，底层基于 Thrift 和自研的 Netpoll 网络库，非常适合高并发、低延迟场景，这正契合消息队列对性能的要求；
 第二，gRPC 的优势在跨语言和 HTTP/2 通信，但我们的 MQ 主要是 Go 内部请求响应场景，并不需要这些复杂功能，所以 Kitex 更轻量、性能更优。”

**在我的项目中，那哪里都用到了rpc呢？**

生产者向broker发送消息

消费者到broker消费消息

broker和zookeeper

leader broker到follower broker的raft通信

**fetch中，要是节点宕机了怎么半？**

**你的实现中有快照机制吗？**

有的，他主要解决了两个问题，第一个是日志无限增长带来的内存消耗以及新加入或者落后太多的节点需要同步大量历史日志，我的代码具体是这样实现的，系统会监控每个broker里面日志的大小，要是超过一定阈值，就会触发快照创建，快照主要包含当前消费者状态，命令状态等，通过序列化保存，创建快照后，raft会删除该索引之前的所有日志，只保留一个虚拟条目记录快照信息，这个快照信息里面就包含了之前删除的哪些日志的最终执行结果，这样就实现了日志压缩。那么要是有新节点来同步日志的时候，leader直接发送快照而不是历史日志，达到提高了同步效率

便于理解：

ai:在我的实现中，快照压缩的是 **Raft 层面的命令日志**，不是业务消息本身。
 当消息已经被持久化到磁盘后，这些命令就没有必要继续保留。
 快照会记录每个 Topic 的最新 offset、消费位点、以及文件状态，然后安全地删除旧日志，从而降低存储开销并加快节点同步速度。

**那你说你的mq仿照了kafka,你仿照了他的哪些地方，比kafka的改进在哪里，有哪些不足可以在进行优化？**

架构层面基本和kafka一样，采用topic-partition两级分区模型，每个topic有多个partition来实现水平扩展，客户端的生产者和消费者分离，生产者负责生产消息，消费者负责消费消息，两者会不影响，设计多个broker节点组成集群，每个partition有一个leader和多个follower实现高可用和容错。在消息层面，我也是采用append-only的日志结构，保证顺序写入的高性能。比kafka改进的地方是存储是的索引机制用的是ssatble索引，而kafka用的是稀疏索引，用raft算法来代替kafka的isr机制，kafka的isr机制依赖网络实现同步，在网络分区是数据可能不一样，但是raft可以保证数据的强一致性。还有就是kafka之前用的是zookeeper,现在也已经变成kraft了。这样做主要是为了实现了一个简化的功能，然后要是要对我的ma进行优化，我会给他将上sendfile零拷贝减少cpu开销，对于消息我会支持ttl,自动清除过期消息

**你是怎样实现持久化的？**

首先我是采用了append-only的顺序化写入方式，新消息总是准驾到文件末尾，提高了磁盘i/o性能；然后就是我实现了批量消息刷盘操作，生产的消息先写到内存队列里面，到达一定数量的时候会进行刷盘，减少频繁刷盘带来的性能损耗；

那么他肯定回问，那要是在消息写入内存队列还没来得及刷到磁盘电脑断电了怎么办？

对于我写的目前要是只是存放在内存队列没来得及放到磁盘的消息是会丢失的，要是怎么改进我觉得可以加入wal与写日志来解决，先写入日志在写数据，确保消息不丢失。

具体做法：在客户端发来消息时，先写入到wal日志中，然后存入到内存队列，数据到达一定后进行刷盘操作

按这样的话，写入wal不就是写入磁盘，那为什么不直接写入磁盘，这样还比较麻烦

因为磁盘写是随机的，效率比较低；而wal是顺序追加写；然后就是你要是每次写都要fsync立马刷盘，但是按照这样的话，先顺序追加到wal,然后写入内存队列，批量刷盘，减少了i/o次数

注意这还有这一点：WAL 写入时，通常只是顺序追加到磁盘的文件缓冲区（页缓存），**并不会立即调用 fsync**【那我就感觉很奇怪了，因为我也是write写入文件，那就说明他不一定写入磁盘，可能是写入到文件缓冲区了，那这一点还需要强调码】

你把消息都存入磁盘，那要是磁盘被占满了该怎么办？

目前者也是我mq的一个改进点，可以用过ttl【time-to-live】机制定期删除过期消息，或者实现消息的压缩和归档

ttl详细设计：在开启mq的时候就开启一个后台线程，每个消息结构体里面都存有他的过期时间，把这些消息按照最小堆的形式来存储，这样子最早过期的消息就在堆顶，一定时间会将堆顶的过期时间和当前时间做对比，要是小于，就进行删除，并继续检查新的堆顶，直到堆顶消息未过期为止。

**你的一致性哈希是怎么做的，为什么要这样做**

他主要解决了传统哈希在节点数量变化是大量数据迁移问题，因为传统做法hash(key)%N,N变了几乎所有的key都要重新分布。我在我的项目中使用一致性哈希是为了实现数据分片和负载均衡。首先他是有一个哈希环的概念，将节点和数据都映射到环上面，并为一个物理节点创建多个虚拟节点来提高负载均衡，来防止节点在环删该分布不均匀而导致的偏斜问题。他通过将数据也就是我项目中的partition进行哈希找到大于等于大的虚拟节点然后对应处真实的物理节点来实现了映射。

举例：要是先在这个环上面有abc三个节点，因为key进行哈希是随机非配的，那么要是落在ab之间就属于b节点，要是落在bc之间就属于c节点，要是落在ca之间就属于a节点，但是要是在ab之间加了一的d节点，那么d节点会夺取属于b节点的key,为了分散说几个节点的key,就加入了虚拟节点

这个环的“大小”其实就是哈希函数输出的范围

ai优化：

在我的项目里我用了一致性哈希来做数据分片和负载均衡。传统做法是 `hash(key) % N`，一旦节点数 N 发生变化，几乎所有 key 的映射都会改变，导致大规模数据迁移。一致性哈希通过引入 **哈希环**，把节点和数据都映射到环上，数据会顺时针找到第一个节点作为归属节点。这样节点增减时，只会影响环上一小段区间的数据。为了避免节点分布不均导致的负载倾斜，我还使用了 **虚拟节点**，即为每个物理节点映射多个点到环上，让数据分布更加均衡。最终实现了在动态扩缩容场景下，既减少了数据迁移量，又保证了节点间负载均衡。

**kafka和pulsar有什么区别？**

他们的主要区别体现在topic持久化和消息去重机制上面，kafka的topic是持久化存储，但是pulsar有持久化topic和非持久化topic,非持久化topic存在在内存，要是broker宕机或者consumer故障消息会丢是，适合低延迟但可容忍消息丢失的场景。在消息去重方面，kafka是通过幂等性来确保消息至少被生产一次，但是pulsar是通过broker端去重，生产者在发送消息给broker的时候会产生一个id,broker在去重缓存里面去找是否由这个id,要是没有就证明是新消息，将他加入到去重缓存里面并存储到队列，要是缓存命中，那么说明这是重复消息就什么也不做。

**raft和redis的gossip有什么区别吗？**

raft是一种基于leader的强一致性，保证所有节点在同一时刻对日志和数据的顺序一致，而gossip是一种去中心化的状态传播协议，他没有leader,节点之间是相互平等的，值保证最终一致性。

**为什么选择Raft而不是Paxos？Raft和Paxos的区别是什么？**

raft和paxos都是分布式一致性算法，目的都是在不可靠网络和节点可能宕机情况下，保证多数节点能达成一致。paxos他是最早的理论基础，证明了一致性时可以实现的，但是算法比较抽象难懂。raft可以看作是paxos的改进，他把一致性问题拆分成leader选举，日志复制河安全性保证这三个子问题，逻辑比较清晰。两者的主要区别在于，paxos有proposer,acceptor,learner三类角色，而raft明确有leadre,follower和candidadate，还有一个区别是，paxos本身只解决单值一致性问题，要支持日值赋值还需要扩展为multi-paxos，逻辑更复杂，而raft天然支持日志复制，也算是一个优点

**你的系统和Kafka、RabbitMQ相比，有什么优势？**

**讲一下ptp和psb模式**

我认为他们主要有两点不同，第一点，消息传递的方式，客户端生产者将消息发送到特定队列，只有一个消费者可以从该队列消费消息，消息一旦被消费，就会从队列删除(那么瓷盘中呢？)；要是psb方式，客户端生产者生产的消息发布到特定主题的某个分区，多个消费者可以订阅该主题，每个订阅者都会受到消息的副本，消息被消费后，也不会立即删除；第二个点是消费者数量，ptp一个消息只会被一个消费者消费，要是多个消费者同时监听一个队列，那么也只会有一个消费者能成功获取消息，其他的都不可以；而psb,只要订阅这个topic的消费者就可以得到消息的副本

**raft和zab有什么区别呢？为什么用raft而不用zookeeper的zab?**

Raft 和 ZAB 都是分布式一致性协议。ZAB 是 ZooKeeper 内部的协议，主要用于元数据和事件广播，它适合低频、全局性的协调任务，比如节点注册、Topic 配置、订阅关系。而 Raft 是一个通用一致性算法，适合高频的日志复制和强一致性场景，所以在我的 MQ 里，我们用 ZooKeeper+ZAB 处理元数据一致性，用 Raft 来保证 Partition 内的消息一致性。这种分层设计不仅避免了ZooKeeper过载，还实现了专业分工：ZooKeeper专注于它擅长的元数据协调，Raft专注于高性能的数据复制，各自发挥最大优势

**你在写你的mq的时候遇到什么难点了吗，你是怎么解决的，有什么收获？**

我认为最大的难点是raft以值性算法，他的理论确实比较容易看懂，但是对理论的理解和工程实现还是有很大的差异的。比如在处理leader选举边界的情况，遇到网络分区，节点故障之后又重启等就会时的逻辑变得复杂，还有快照机制，实现日志压缩。这是第一个问题，也就是raft内部的特殊情况的处理，还有就是我时给每一个partition维护一个raft集群的，也就是需要管理多raft集群，那么就需要zookeeper来多这些raft进行管理对我来说也是一个挑战；然后就是在进行查找数据，我发现逐个遍历他的效率太低了，因此就参凯稀疏索引实现了批量写入磁盘并在消息块前面加上块索引来实现一个又

**cap**

c【consistency】一致性：所有节点在同一时间看到的数据是一致的

a【accessy】可用性：每个请求都那哪个在有限时间内返回结果

p[partition telorat]分区容忍性：系统在网络分区的情况下仍然可以继续提供服务

最多同时满足两项。在网络分区出现时，系统必须在一致性和可用性之间做权衡。CP 系统保证一致性，可能牺牲可用性；AP 系统保证可用性，可能牺牲一致性。

我的mq和kafka是一样的实现了cp

c:raft提供的强一致性.他选择牺牲了部分可用性。在某个partition下的leader宕机了，那么此时会停止改分区的读写，该分区（Raft集群）会通过Raft协议内部的选举流程选出新的Leader。一旦新的Leader选出，它会更新Zookeeper中的元数据。在此期间，该分区会暂停对外提供写服务，从而牺牲了该分区的可用性以保证一致性。客户端会通过Zookeeper获取到最新的Leader信息，然后恢复对新Leader的读写

p:系统能在出现**网络分区**（节点之间无法通信）时继续对外提供服务。

自己实现的Raft和标准的raft一样吗

遇到的难点怎样解决的

braft[百度]

**死信队列**

他是一种特殊的消息队列，存储无法被正常消费的消息。

本来他是在正常队列，但是要时出现，消息被拒绝，消息过期或者是队列满了。就会将他加入到死信队列，

**延迟队列**

放进去的消息不会被立马消费，而是会等到设置时间之后在进行处理。

常用的场景：订单30分钟没支付自动取消

实现方式：

redis的zset,通过score存放时间戳，定时查询到期时间

也可以通过时间轮来实现

kafka的多topic方案，提前船舰多个延迟topic,每个代表一种延迟时间，生产者会根据业务需要，把消息发送到队赢得延迟topic,然后会创建一个后台线程，他会定期查看所有的延迟队列，检查消息是否到期，钥要是到期，就将消息转发到真实

ttl队列：为每个消息设置一个过期时间，消息到达ttl时间后，要是还没有被消费，就被看作过期时间，被丢弃或者转到死信队列里面。像rabbit塔内部每条消息都维护一个ttl，每次取消息的时候都判断他是否过期，要是过期会进行标记。

**为什么要有term,他的作用是什么，可以无限增长吗？**

他像一个逻辑时钟，主要作用是保证raft算法的正确性，他是怎样保证的呢？term是一个单调递增的证书，每次进行raft选举的时候，都会产生一个新的term,这也就说明，在人以给定的term内，最多只会有一个leader,也就是通过term可以唯一标识出当前的leader,然后他还可以解决日志冲突以及日志一致性。在raft进行日志复制的过程，会先比较term的大小，要是fllower的小于leader，那就就可以直接进行日志的复制；要是大于，就直接拒绝，leader也知道自己已经落后，就会将降级为follower

按照理论来说日志时可以无限增长的，因为我的mq比较小,我使用int来存储的，在集群稳定的情况况下实际上他增长的会非常缓慢

**为什么每个log条目都需要term和index**

logterm:

每个条目都包含logterm,他标识生成该条目时leader所处的任期号。

term的作用主要时保证raft的安全性和一致性，在同一任期内最多只能有一个leader,同时在日志复制过程种，term可以用来叛逆端日志条目的新旧以及减限额冲突，比如两个条目index相同，那么term大的会被留下

index:

他主要时日志条目在整个日志序列种的位置，时单调递增的整数。

他会在leader给follower同步日志的时候

kafka的rebalance【分区在均衡】

kafka的高水位问题

测试：

**你有没有压测过你的mq,能达到多少吞吐量？**

因为我的mq比较小，我只测试过一些简单的，当时是跑了三个节点，然后我用ai给我写了一个简单的测压脚本，具体就是模拟10个生产者并发写入消息，每条消息1kb左有大小，然后维护一个计数器，这个计数器主要是在生产消息后进行+1操作，最后统计整体吞吐。大概是能跑每秒两万条消息

**消息队列解决了什么问题？讲一下他的应用场景（消息队列）**

消息队列时用来在系统之间异步传递消息的中间件，主要解决解耦，异步处理和流量消峰的问题

解耦：

比如说用户下单一个东西，此时会产生一系列的操作，扣库存，生成订单，发短信，通知物流，要是一个链路全步走完，那么耦合太严重了，要是其中有某个服务器挂了，全链路阻塞。

现在有了消息队列，那么订单服务只负责发送消息，其他服务异步处理，提高稳定性

异步：比如说用户注册邮箱，需要发送欢迎邮件，要是直接同步发送，可能需要响应几秒才发送；要是注册接口写入消息队列，那么可以先直接返回欢迎，邮箱注册的流程异步返回

消峰：在双十一这种高峰期，库存服务能处理的请求有限，可以将高峰期请求写入队列，服务器按自身能力逐步消费，起到了缓冲层作用，保证服务器不会被压垮

ai:消息队列是用来在系统之间**异步传递消息**的中间件，主要解决三个问题：**解耦、异步和削峰填谷**。

首先是**解耦**。比如用户下单后要扣库存、发短信、通知物流等，如果所有逻辑都在一个链路中执行，耦合严重且风险高。一旦某个服务挂掉，整个流程就会阻塞。
 有了消息队列后，订单服务只需要发送一条消息，其他服务各自异步消费消息，实现了模块间的解耦和高可用。

第二是**异步处理**。像用户注册发送邮件这种耗时操作，不必同步执行。注册接口只需把消息写入队列，系统立即返回，邮件服务异步消费消息去发送邮件，从而提升了系统响应速度和用户体验。

第三是**削峰填谷**。比如在双十一大促时，瞬时订单量激增，库存系统可能扛不住。通过消息队列，把高峰期的请求暂存起来，消费者按自己的能力逐步处理，起到一个缓冲层的作用，防止系统被压垮。

**raft和zab有什么区别**

raft和zab都是实现分布式一致性的协议。他们的侧重点不同，raft时一种通用的分布式一致性算法，适合日志复制，分布式数据库等场景。而zab时专门为zookeeper设计的原子广播协议，侧重顺序一致和恢复。raft强调线性一致性，使用term+过半的投票选举机制，zab强调顺序一致性，新leader选举时希须保证拥有最新的记录，还有就是我记得raft只要过半就可以进行提交，而zab时两阶段提交模型

Raft 和 ZAB 都是分布式一致性协议，但它们的设计目标和侧重点不同。

**Raft** 是一种通用的分布式一致性算法，主要用于实现复制状态机，比如 etcd、TiKV 这样的分布式数据库。它强调的是 **线性一致性（Linearizability）**，通过 **term + 过半投票机制** 来选举 Leader，并且在日志复制时，只要超过半数节点确认，就可以提交日志。

**ZAB（ZooKeeper Atomic Broadcast）** 则是专门为 **ZooKeeper** 设计的原子广播协议，侧重于 **顺序一致性（Sequential Consistency）** 和系统在崩溃后的**恢复**。它要求新选出的 Leader 必须拥有最新的事务记录，并且采用 **两阶段提交（2PC）** 的广播机制来保证事务的可靠提交。

**那你除了kafka之外还知道什么其他的消息队列吗，他们都有什么特点**

ai:

除了 Kafka，常见的消息队列还有 RabbitMQ、RocketMQ、ActiveMQ 和 Pulsar。
 RabbitMQ 基于 AMQP 协议，功能丰富但性能一般，适合企业级业务解耦；
 RocketMQ 是阿里开源的高性能 MQ，支持事务消息和延迟队列；
 ActiveMQ 较老，轻量易用但性能不高；
 Pulsar 是新一代云原生消息系统，支持多租户和超大规模部署。
 相比之下，Kafka 更适合高吞吐场景，比如日志流、实时计算。

**网络分区脑裂问题**

我理解的分区脑裂问题时由于网络故障，集群被分割成了多了独立的子集，每个自己都肯哪个选出自己的leader,导致树苴严重不一致的问题。而我的mq因为他是通过raft共式算法，他的多数派原则可以保证，raft要求任何leader的产生都需要超过一半节点的投票，也就是说在出现网络分区的时候，最多只有一个分区能够获得多数节点，从而成为合法的 Leader。 其他分区由于票数不够，会保持在 Follower 状态，这样就避免了多个 Leader 同时存在的情况，也就解决了脑裂问题。

**那面时官可能会问，那要是网络分区，那有一个分区因为一致达不到而一直时竞选者，那么这会导致哪个分区接将选者节点的term一直重复+1,导致在恢复网络分区之后的什么问题，需要怎么解决这个问题？**

在网络分区的情况下，要是某个分区达不到多票数，他在选举超时后会反复发起新的选举请求，每次选举都会导致term+1,者会导致term不断递增。那么在网络恢复之后，这些孤立节点的term会比原leader的term高，那么按照raft规则，term更高的节点会让就leader自动退位，这会导致频繁的leader切换

那么解决方案：可以加入预选举机制：也就是在节点真正term+1的时候，会先发起一轮预投票，也就是他先询问其他节点是否会投票给自己，但后提前看他的投票数量是否过半，要是不过半就不将自己的term+1,那么在网络恢复之后，leader会向他们发送心跳，这些鼓励的节点发现自己落后，就会更新自己的term并返回follower状态

Raft 还通过 **term（任期号）机制** 保证安全性。
 一旦旧的 Leader 在网络恢复后发现自己的 term 落后，就会自动降级为 Follower，避免出现旧 Leader 继续写入数据的情况

**应用的状态机是具体时怎么应用的**

> - 状态机会把这条消息追加到 Partition 的内存队列 / 消息块中，保证每个节点的 Partition 状态一致；
> - 当 Consumer 拉取消息时，也是从状态机的 Partition 中读取，确保读取的是一致的消息。

**讲一下kafka的事务？**

etcd对于raft的优化有哪些

raft选举的时候的持久化？

如何设计业务监控指标？

报警风暴？

为什么选用raft而不是kafka的isr?

**怎样优化**

可以向就是在follower页可以进行读操作：租约读和安全读

就是leader给自己发一个租约，在这个租约内客户端要是是读取消息直接可以在follower上面进行读取，但是他又租约时间，要是leader崩溃后租约没有失效，此时还是在follower进行读取，肯哪个会产生·读到就数据的情况

安全读：

在客户端读取消息的时候会向leader发送一次心跳，心跳含有自己的最新index,要是leader节点回了信息，就可以继续读了，比较适合于强一致性业务
