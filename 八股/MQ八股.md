MQ

**面试管说，那你讲一下你mq的实现,你应该怎么说**

面试管您好，MQ是我开发的一个分布式消息中间件项目，目前基本支持分布式消息传递，RAft共识算法，zookeeper元数据管理，sstable存储引擎，高可用集群等核心功能，现在我详细讲一下我项目的整个流程。整个系统分为三层，分别是zookeeper元数据层做动态配置，Broker消息处理层，Client客户端层；Zookeeper元数据层，负责 Topic,Partition,Broker状态等配置管理以及故障检测与选举的协调，在Broker消息处理层，负责消息的写入，存储，副本同步和分发。每个Broker上的Partition内部有Raft协议来保证一致性。在Client客户端层，Producer和Consumer通过RPC与Broker交互，支持点对点和发布订阅。

**单点故障怎么办？**

首先在我的mq中有两个心跳机制，要是没有按时收到心跳，那么就说明有故障产生了。第一个 leader broker会定时向订阅他下面这个partition的消费者发送心跳，消费者要是没在一定时间内进行回复，那么leader broker会认为这个消费者挂了，他就会将这个消费者的扎状态由alive编成down,，那么要是有新消息到来时，就不会退送给这个消费者了；那么还有就是leader broker会定时向下面的follower broker发送心跳，要是follower在定时内没有收到leader broker的心跳，那么follower broker会认为leader broker已经宕机了，那么他会变成竞选者像其他follower发送投票请求，当收到一半同意时他就会变成leader

**1：你是如何保证数据的一致性和高可用性的？**

首先我通过raft共识算法，每个Partition都有一个独立的Raft集群，保证写入数据的强一致性；要是Leadre发生宕机，Follower会在超时后发起选举，选举出新的Leader即可，为了防止多个Follower同时发起选举，采用选举超时随机化来提高选举成功率。写入消息时，可以选择ack，Leader先会写入本地日志，在复制给Follower,然后返回；添加了快照机制。

你的mq是怎样实现高可用高可用性的？

首先raft的多数派原则保证只要超过一般节点存活，集群就继续工作，即使leader宕机，follower会在超时后自动发起选举，快速完成新的leader,完成自动故障转移；其次，数据写入时必须复制到多数派节点才算提交，这样可以保证即使少数节点当即，数据也不会丢失，新leader仍然可以提供服务。

**你的mq是怎么实现强一致性的？**

raft的一致性问题可以分成3个子问题，分别是leader选举，日志同步和安全性保证。对于leader选举，leader会定时向follower发送心跳，follower内部会维护一个随机超时时间的定时器，当在定时器超时之前收到来自leader的心跳，会将定时器时间置0并继续保持follower状态,要是在定时器超时没有收到来自leader的心跳，那么follower会认为leader宕机，follower会变成竞选者，然后follower会将自己的term+1,向其他follower发送自己想成为leader的请求，携带自己日志的term和index,其他follower进行检查，看是否他的term比自己的大，要是一样，就继续比较最后一条日志的term和index,要是满足投票条件，follower会向候选者发送投票，要是收到超过一半follower的同意，那么他就会成为leader并通知下面的follower自己成为了leader.对于日志同步，Leader将客户端请求加入到自己的日志中，然后并行向其他follower发送请求，其他follower将请求写入自己的日志之后并给leader回复自己已经复制，当leader接收到超过一半follower已经进行复制，那么这条日志会被标记为commit,接着他会执行这个请求并将执行结果返回给客户端；然后就是安全性保障，raft的多数投票规则，提交规则保证了raft的安全，他又两点，已经commit的消息一定会存在在后续leader节点中，但是没有commit的消息，就可能会丢失



**2：你是怎样优化存储引擎的？**

存储层面的结构是通过broker/topic/partition/block文件的层及，这样比较方便查找；我实现了类似LSM-Tree的存储结构，每个消息块里面有很多条消息，每个消息块前面有固定24字节的索引头，包含起始index,结束index,以及数据大小。这样消费者在查询offset消息是，不用从每条消息开始遍历，而是通过消息头进行块嗣定为，将查找复杂度直接从o(n)降低到了O(m);因为他是以追加的方式写入磁盘的，因此当消费者offset找的时候也比较好找；还有就是在存储的时候先写进内存队列，积累到消息阈值会将他进行刷盘操作，这样保证了性能又提高了安全；

**消息的发送机制**

生产者要发送消息时，他会先根据topic+partition来在本地先找有没有存储相对应的leaderbroker,要是没有，那么他会通过GetLeader来去zookeeper中找（这里zookeeper是怎么通过topic+partition来找到相应的leader broker的?___zookeeper他是通过topic+partition+nowblock拉获取到最新正在写的块的结构体，这里面就有对应的leader block)，找到后与leader broker建立连接，然后向这个leader broker发送消息,发送消息的时候我们会戴上ack,要是ack=-1,表示需要大多数follower同步复制并确认确认，根据ack=1是只要leader写入成功即可，ack=0则是开启一个携程进行异步副本写入操作。我们则一般选择的是ack=-1的形式。直接返回leader broker会先查询topic和partition是否存在，不存在就创建并启动partition接收消息，然后leader broker获得他下面的Followe broker,将消息发送给他们，他们将消息存储在本地并返回给leadre broker,leader broker看到有超过一半存储到partition 副本成功改就也存储在自己的partition下面，然后将结果返回给客户端生产者

**要是面试官问，那你raft同步和异步(fetch的实现)**

就是在producer的时候，就会和zookeeper进行交互设置他的模式为fetch和副本数量，那么在zookeeper他会通过topic+partition来通过一致性哈希来获取到broker,然后将第一个broker当作leader broker,然后遍历 follower，向每个follower发送addfetch的命令，这里面传入的参数就有 laeder broker和所有的follower broker，每个follower broker再收到命令后，会判断是否是leader broker,要是是他会遍历其他的follower broker，准备接收其他follower的pull请求，要是是Follower broker,他会和leader broker建立连接去主动拉取消息

那么就是要是是fetch,那么就要用到一致性哈希，他是通过topic+partition来得到一个key,然后通过这个key得到paroducer需要的节点数量，但是现在leader broker会变，他会变成第一个获取到的 broker，

**那你是怎样通过一致性哈希选出用哪个broker的**

首先就是根据消息带着的topic+partition(这里需要在细心，你还不清楚是怎样算的)计算出消息key的哈希值，然后在哈希环上找大于等于哈希值最近的虚拟节点，虚拟节点对应真实的broker就是负责存储该消息的broker

**为什么要虚拟节点：**

通过一个broker对应多个虚拟节点，并将这些虚拟节点均匀的分布在哈希环上面，可以使数据更加均匀的分布在不同的broker(那么又有一问了，那leader节点呢，没用吗？和他们一样吗？）上面，当某个broker宕机时，我们只需要将他对应的虚拟节点删除，并把虚拟节点上的数据迁移到其他节点就行。主要提高负载均衡能力和容错性

**那对于消费者消费消息呢？整个流程**



**怎时使得消息不丢失**

**如何保证消息只被消费一次？**

ai:为了保证消息只被消费一次，针对不同的消息模式有不同的策略。在点对点（PTP）模式下，通过独占消费确保每个 partition 只能被一个 consumer 消费，顺序消费避免重复，原子更新 offset 保证消费进度正确，故障恢复后 consumer 从上次位置继续。在发布订阅（PSB）模式下，每个 consumer 维护独立的 offset，同一组内只有一个 consumer，不同组可以并行消费，消费进度持久化到 Zookeeper。通用的保证机制包括消息 ID 唯一性去重、状态跟踪（NOTDO -> HAVEDO -> HADDO）、确认机制确保消费成功才更新 offset、故障处理时检测故障并重新分配，以及持久化存储消费进度到 Zookeeper。

**kafka是怎样通过米等性来实现生产者只生产一次消息的？**

核心在于为每个生产者分配一个唯一的pid,针对每个topic+partition维护一个递增的序列号，当生产这发送消息是,broker会验证这个序列号是否是与环村中最新序列号连续，要是连续就进行更新并存储，要是重复就直接丢弃

**了解ack机制**

**消费者的推拉有什么区别**

LSM-Tree架构 = MemTable + 多层SSTable + Compaction机制

我实现了一个sstable存储格式

**那你讲一下lsm-tree**

是一种写入优化的存储结构，他的核心思想是写入内存，顺序写磁盘，合并来维护有序性。整体流程大概是这样，首先将数据顺序写入到wal文件中，防止突然断电导致消息丢失的情况，然后在将数据写入到memtable这个有序的红黑数中，当memtable写满后会进行刷盘操作也就是在存盘存储成sstable文件；要是你要查询一个数据，他会先查memtable,在查wal,要是wal没有，那么就说明肯定是很久远的消息，那么他会通过布隆过滤器来查询这个消息key可能在哪个sstable文件，因为一个key可能在多个sstable中，然后再这些sstable中用稀疏索引来快速定位文件内部的消息，然后就是合并，后台会将多个sstable合并成更大的sstable来减少文件的数量，并清理旧版本的key

**你能说一下你的和lsm-tree有什么区别吗？**

我实现的主要是ssatble格式，他是lsm-tree架构中的一个核心组件，我借鉴了lsm-tree设计思想，实现了主要的消息块索引，顺序写入等功能，我只是将数据块顺序存储，但是lsm-tree的MemTable用跳表或红黑树等有序结构存储新写入的数据，还缺少lsm-tree关键的wal机制,这个主要是确保新写入的数据先记录到wal里面，即使断电了，也不会因为放在内存队列而消失的情况；以及lsm-tree还实现了多层结构会将文件从小的合并成大的，这样避免了文件数量一直增长的情况

**你是如何实现负载均衡和水平扩展的**

**如何解决消息堆积问题？消费者消费速度跟不上怎么办？**

可以从预防和解决两个角度触发解决消息挤压这个问题，对于预防，可以优化生产端，添加限流控制；也可以优化消费端，可以减少消费端不必要的i/o操作或者增加分区数量也就是体改并行能力；对于解决，可以直接扩大消费者数量

 **网络分区时，你的系统如何处理？会出现脑裂吗？**

**为什么选择kitex而不是gRPC？**

kitex和grp都是为了实现rpc框架，我实现的是分布式消息队列，各个系统之间需要通信，而rpc框架就可以让我向调用本地函数一样调用远程服务。

**在我的项目中，那哪里都用到了rpc呢？**

生产者向broker发送消息

消费者到broker消费消息

leader broker到follower broker的raft通信

在我的mq项目中选择kitex而不选择grpc,首先是因为我的mq是用go实现的， Kitex 是专门为 Go 生态设计的高性能 RPC 框架；而且kitex他主要基于thrift写一和自即研究的netpoll网络库，在高并发，高体那兔场景夏延迟耕地，性能更优，非常契合消息队列这种对性能要求高的系统。对于grpc他主要是在跨语言和http/2通信的优势，对于我们mq的简单的请求响应，不需要实现这么复杂的，所以我觉得选用kitex更合适

**你的实现中有快照机制吗？**

有的，他主要解决了两个问题，第一个是日志无限增长带来的内存消耗以及新加入或者落后太多的节点需要同步大量历史日志，我的代码具体是这样实现的，系统会监控每个broker里面日志的大小，要是超过一定阈值，就会触发快照创建，快照主要包含当前消费者状态，命令状态等，通过序列化保存，创建快照后，raft会删除该索引之前的所有日志，只保留一个虚拟条目记录快照信息，这个快照信息里面就包含了之前删除的哪些日志的最终执行结果，这样就实现了日志压缩。那么要是有新节点来同步日志的时候，leader直接发送快照而不是历史日志，达到提高了同步效率

**那你说你的mq仿照了kafka,你仿照了他的哪些地方，比kafka的改进在哪里，有哪些不足可以在进行优化？**

架构层面基本和kafka一样，采用topic-partition两级分区模型，每个topic有多个partition来实现水平扩展，客户端的生产者和消费者分离，生产者负责生产消息，消费者负责消费消息，两者会不影响，设计多个broker节点组成集群，每个partition有一个leader和多个follower实现高可用和容错。在消息层面，我也是采用append-only的日志结构，保证顺序写入的高性能。比kafka改进的地方是存储是的索引机制用的是ssatble索引，而kafka用的是稀疏索引，用raft算法来代替kafka的isr机制，kafka的isr机制依赖网络实现同步，在网络分区是数据可能不一样，但是raft可以保证数据的强一致性。还有就是kafka之前用的是zookeeper,现在也已经变成kraft了。这样做主要是为了实现了一个简化的功能，然后要是要对我的ma进行优化，我会给他将上sendfile零拷贝减少cpu开销，对于消息我会支持ttl,自动清除过期消息

**你是怎样实现持久化的？**

首先我是采用了append-only的顺序化写入方式，新消息总是准驾到文件末尾，提高了磁盘i/o性能；然后就是我实现了批量消息刷盘操作，生产的消息先写到内存队列里面，到达一定数量的时候会进行刷盘，减少频繁刷盘带来的性能损耗；

那么他肯定回问，那要是在消息写入内存队列还没来得及刷到磁盘电脑断电了怎么办？

对于我写的目前要是只是存放在内存队列没来得及放到磁盘的消息是会丢失的，要是怎么改进我觉得可以加入wal与写日志来解决，先写入日志在写数据，确保消息不丢失

你把消息都存入磁盘，那要是磁盘被占满了该怎么办？

目前者也是我mq的一个改进点，可以用过ttl机制定期删除过期消息，或者实现消息的压缩和归档

**你的一致性哈希是怎么做的，为什么要这样做**

他主要解决了传统哈希在节点数量变化是大量数据迁移问题，因为传统做法hash(key)%N,N变了几乎所有的key都要重新分布。我在我的项目中使用一致性哈希是为了实现数据分片和负载均衡。首先他是有一个哈希环的概念，将节点和数据都映射到环上面，并为一个物理节点创建多个虚拟节点来提高负载均衡，来防止节点在环删该分布不均匀而导致的偏斜问题。他通过将数据也就是我项目中的partition进行哈希找到大于等于大的虚拟节点然后对应处真实的物理节点来实现了映射。

ai优化：

在我的项目里我用了一致性哈希来做数据分片和负载均衡。传统做法是 `hash(key) % N`，一旦节点数 N 发生变化，几乎所有 key 的映射都会改变，导致大规模数据迁移。一致性哈希通过引入 **哈希环**，把节点和数据都映射到环上，数据会顺时针找到第一个节点作为归属节点。这样节点增减时，只会影响环上一小段区间的数据。为了避免节点分布不均导致的负载倾斜，我还使用了 **虚拟节点**，即为每个物理节点映射多个点到环上，让数据分布更加均衡。最终实现了在动态扩缩容场景下，既减少了数据迁移量，又保证了节点间负载均衡。

**kafka和pulsar有什么区别？**

他们的主要区别体现在topic持久化和消息去重机制上面，kafka的topic是持久化存储，但是pulsar有持久化topic和非持久化topic,非持久化topic存在在内存，要是broker宕机或者consumer故障消息会丢是，适合低延迟但可容忍消息丢失的场景。在消息去重方面，kafka是通过幂等性来确保消息至少被生产一次，但是pulsar是通过broker端去重，生产者在发送消息给broker的时候会产生一个id,broker在去重缓存里面去找是否由这个id,要是没有就证明是新消息，将他加入到去重缓存里面并存储到队列，要是缓存命中，那么说明这是重复消息就什么也不做。

**raft和redis的gossip有什么区别吗？**

raft是一种基于leader的强一致性，保证所有节点在同一时刻对日志和数据的顺序一致，而gossip是一种去中心化的状态传播协议，他没有leader,节点之间是相互平等的，值保证最终一致性。

**为什么选择Raft而不是Paxos？Raft和Paxos的区别是什么？**

raft和paxos都是分布式一致性算法，目的都是在不可靠网络和节点可能宕机情况下，保证多数节点能达成一致。paxos他是最早的理论基础，证明了一致性时可以实现的，但是算法比较抽象难懂。raft可以看作是paxos的改进，他把一致性问题拆分成leader选举，日志复制河安全性保证这三个子问题，逻辑比较清晰。两者的主要区别在于，paxos有proposer,acceptor,learner三类角色，而raft明确有leadre,follower和candidadate，还有一个区别是，paxos本身只解决单值一致性问题，要支持日值赋值还需要扩展为multi-paxos，逻辑更复杂，而raft天然支持日志复制，也算是一个优点

**你的系统和Kafka、RabbitMQ相比，有什么优势？**

**讲一下ptp和psb模式**

我认为他们主要有两点不同，第一点，消息传递的方式，客户端生产者将消息发送到特定队列，只有一个消费者可以从该队列消费消息，消息一旦被消费，就会从队列删除(那么瓷盘中呢？)；要是psb方式，客户端生产者生产的消息发布到特定主题的某个分区，多个消费者可以订阅该主题，每个订阅者都会受到消息的副本，消息被消费后，也不会立即删除；第二个点是消费者数量，ptp一个消息只会被一个消费者消费，要是多个消费者同时监听一个队列，那么也只会有一个消费者能成功获取消息，其他的都不可以；而psb,只要订阅这个topic的消费者就可以得到消息的副本

**raft和zab有什么区别呢？为什么用raft而不用zookeeper的zab?**

Raft 和 ZAB 都是分布式一致性协议。ZAB 是 ZooKeeper 内部的协议，主要用于元数据和事件广播，它适合低频、全局性的协调任务，比如节点注册、Topic 配置、订阅关系。而 Raft 是一个通用一致性算法，适合高频的日志复制和强一致性场景，所以在我的 MQ 里，我们用 ZooKeeper+ZAB 处理元数据一致性，用 Raft 来保证 Partition 内的消息一致性。这种分层设计不仅避免了ZooKeeper过载，还实现了专业分工：ZooKeeper专注于它擅长的元数据协调，Raft专注于高性能的数据复制，各自发挥最大优势

**你在写你的mq的时候遇到什么难点了吗，你是怎么解决的，有什么收获？**

我认为最大的难点是raft以值性算法，他的理论确实比较容易看懂，但是对理论的理解和工程实现还是有很大的差异的。比如在处理leader选举边界的情况，遇到网络分区，节点故障之后又重启等就会时的逻辑变得复杂，还有快照机制，实现日志压缩。这是第一个问题，也就是raft内部的特殊情况的处理，还有就是我时给每一个partition维护一个raft集群的，也就是需要管理多raft集群，那么就需要zookeeper来多这些raft进行管理对我来说也是一个挑战；然后就是在进行查找数据，我发现逐个遍历他的效率太低了，因此就参凯稀疏索引实现了批量写入磁盘并在消息块前面加上块索引来实现一个又

